\chapter{Set-Covering}

In questo capitolo viene presentato il problema del Set-Covering, che è il problema di riferimento per l'algoritmo
sviluppato nel prossimo capitolo.
\section{Formulazione del Problema}
Siano \( \mathcal{I} \) un insieme di \( m \) elementi e \( \mathcal{F} = \{\mathcal{F}_1, \ldots, \mathcal{F}_n\} \)
una famiglia di sottoinsiemi di \( \mathcal{I} \). Il problema del Set-Covering richiede di trovare la più piccola
collezione
\(
    \mathcal{F}^{\,\text{\raisebox{2pt}{$\star$}}} \!\subseteq \mathcal{F}
\)
tale che
\begin{equation}\label{eq:scpconstr}
    \bigcup_{\mathcal{F}_j \, \in \, \mathcal{F^{\,\text{\raisebox{2pt}{$\star$}}}}} \!\!\mathcal{F}_j = \mathcal{I}
\end{equation}
In altre parole, si richiede che ciascun elemento di \( \mathcal{I} \) sia coperto da almeno uno dei sottoinsiemi in
\(
    \mathcal{F}^{\,\text{\raisebox{2pt}{$\star$}}}.
\)
Il problema può essere formulato come problema di programmazione lineare intera, introducendo, per ciascun elemento in
\(
    \mathcal{F},
\)
una variabile binaria che rappresenta l'appartenenza a
\(
    \mathcal{F}^{\,\text{\raisebox{2pt}{$\star$}}}.
\)
In particolare, definiamo
\begin{equation}
    x_j = \scalebox{1.05}{\bigg\{}
    \begin{array}{@{}ll}
        1 & \text{se } \mathcal{F}_j \in \mathcal{F}^{\,\text{\raisebox{2pt}{$\star$}}} \\
        0 & \text{altrimenti}
    \end{array}\qquad \forall j\colon 1 \leq j \leq n
\end{equation}
A questo punto, le variabili del problema permettono di specificare la funzione obiettivo
\begin{equation}
    \min \;\;\sum_{j = 1}^n x_j
\end{equation}
che minimizza il numero dei sottoinsiemi scelti, ossia la cardinalità di
\(
    \mathcal{F}^{\,\text{\raisebox{2pt}{$\star$}}}.
\)
Infine, dobbiamo specificare la condizione \eqref{eq:scpconstr} come vincolo lineare, e quindi richiediamo che
\begin{equation}\label{eq:dummyconstr}
    \sum_{j\colon i \,\in\, \mathcal{F}_j} \!\!x_j \geq 1 \qquad \forall i \in \mathcal{I}
\end{equation}
Combinando tutte le considerazioni fatte, otteniamo la formulazione matematica
\begin{equation}\label{eq:defscp}
    \left\{
    \begin{array}{lll}
        \min & \displaystyle\sum_{j = 1}^n x_j \\[20pt]
             & \!\!\!\displaystyle\sum_{j\colon i \,\in\, \mathcal{F}_j} \!\!x_j \geq 1 & \forall i \in \mathcal{I} \\[20pt]
             & x_j \in \{0, 1\} & \forall j\colon 1 \leq j \leq n
    \end{array}\right.
\end{equation}
che descrive il problema del Set-Covering come problema di programmazione lineare intera. Inoltre, notiamo che
l'espressione della funzione obiettivo stabilisce implicitamente il limite superiore per le variabili, poiché non c'è
mai convenienza nel selezionare lo stesso elemento \( \mathcal{F}_j \in \mathcal{F}\) più di una volta. Questa
considerazione ci permette di modificare i vincoli di dominio, richiedendo che le variabili siano semplicemente intere
non negative.

\section{Rilassamento Lineare}

Nel seguito di questo lavoro ci limiteremo a considerare il rilassamento lineare del problema del Set-Covering, che
rimuove il vincolo di interezza per le variabili e richiede solamente che siano non negative. Per gli obiettivi di
questo lavoro, è conveniente utilizzare una formulazione alternativa, del tutto equivalente a quella
presentata in \eqref{eq:defscp}, che si ottiene trasformando il vincolo della \eqref{eq:dummyconstr} in

\begin{equation}\label{eq:smartconstr}
    \sum_{j = 1}^n a_{ij}\,x_j \geq 1 \qquad \forall i \in \mathcal{I},
\end{equation}
dove \( a_{ij} \) è un parametro binario che assume il valore 1 se e solo se
\(
    i \in \mathcal{F}_j,
\)
per ogni elemento
\(
    i \in \mathcal{I} \text{, con } 1\leq j \leq n.
\)
A questo punto, possiamo definire il rilassamento lineare del Set-Covering con la formulazione
\begin{equation}\label{eq:scplr}
    \left\{
    \begin{array}{lll}
        \min & \displaystyle\sum_{j = 1}^n x_j \\[20pt]
             & \displaystyle\sum_{j = 1}^n a_{ij}\,x_j \geq 1 & \forall i \in \mathcal{I} \\[20pt]
             & x_j \geq 0 & \forall j\colon 1 \leq j \leq n
    \end{array}\right.
\end{equation}
che è quella di un problema di programmazione lineare nella forma canonica
\begin{equation}\label{eq:scpcanonical}
    \left\{
    \begin{array}{ll}
        \min & \vec{c}^{\tr}\vec{x}\\
        &\vec{A}\vec{x} \geq \vec{b} \\
        &\vec{x} \geq 0
    \end{array}\right.
\end{equation}
ottenuta ponendo \( \vec{A} = [a_{ij}] \in \mathbb{R}^{m\times n}\), con \( \vec{x} = [x_1, \ldots, x_n]^{\tr} \in
\mathbb{R}^n \), \( \vec{c} = [1, 1, \ldots, 1]^{\tr} \in \mathbb{R}^n \) e \( \vec{b} = [1, 1, \ldots, 1]^{\tr} \in
\mathbb{R}^m \).

Naturalmente, una soluzione del rilassamento lineare non è necessariamente ammissibile per il problema di partenza.
Inoltre, per come è definito, il rilassamento lineare non ha nessun significato in relazione al problema del
Set-Covering. Una soluzione in cui una variabile compare con un valore non intero non ha senso, poichè le variabili del
problema iniziale sono state introdotte con un signficato che dipende fortemente dalla presenza del vincolo di
interezza. Infine, anche per il rilassamento lineare rimane valida la considerazione fatta in precedenza, relativamente
al limite superiore per le variabili, che le vincola implicitamente ad assumere valori nell'intervallo \( [0, 1] \).

\subsection{Problema Duale}
Procediamo ora a definire il problema duale per il rilassamento lineare del problema del Set-Covering, seguendo le
considerazioni che abbiamo fatto in
\ref{sec:lpduality}. Introduciamo un vettore \( \vec{u} = [u_1, \ldots, u_m]^{\tr} \in \mathbb{R}^m \) di
moltiplicatori duali per combinare linearmente i vincoli e identificare una limitazione inferiore al valore della
funzione obiettivo. In particolare, dobbiamo trovare
\(
    \vec{u}
\)
tale che
\begin{equation}\label{eq:dualconds}
    \sum_{j = 1}^n x_j \geq \sum_{i = 1}^m \Big(u_i \sum_{j = 1}^n a_{ij}\, x_j \Big) \geq \sum_{i = 1}^m u_i
\end{equation}
A questo punto, per trovare la limitazione migliore possibile, cioè quella più alta, dobbiamo risolvere il problema
duale
\begin{equation}\label{eq:scplrdual}
    \left\{
    \begin{array}{lll}
        \max & \displaystyle\sum_{i = 1}^m u_i \\[20pt]
             & \displaystyle\sum_{i = 1}^m a_{ij}\,u_i \leq 1 & \forall j\colon 1 \leq j \leq n \\[20pt]
             & u_i \geq 0 & \forall i\colon 1 \leq i \leq m
    \end{array}\right.
\end{equation}
che si ottiene dalle disuguaglianze nella \eqref{eq:dualconds}. Notiamo che per come è definito, il vincolo del problema duale
impone una limitazione superiore ai moltiplicatori, ossia \( u_i \in [0, 1] \) per ogni
\(
    1 \leq i \leq m,
\)
visto che
\(
    a_{ij}
\)
è un parametro binario che può assumere solo i valori 0 e 1, per
\(
    1 \leq i \leq m \text{ e } 1 \leq j \leq n.
\)
L'utilità pratica nella definizione del problema duale sarà chiara a breve. La cosa importante da ricordare è la
limitazione che abbiamo ottenuto, relativamente ai possibili valori che i moltiplicatori possono assumere.

\section{Rilassamento Lagrangiano}
Nel corso del primo capitolo abbiamo affrontato l'argomento del rilassamento lagrangiano, relativamente ai problemi di
programmazione lineare. In questa sezione possiamo sfruttare i ragionamenti che abbiamo fatto per applicare questo
concetto alla formulazione \eqref{eq:scplr} del rilassamento lineare del set-covering. Rimuoviamo i vincoli di copertura
per gli elementi in \( \mathcal{I} \) e aggiungiamo la penalizzazione lagrangiana alla funzione obiettivo: introduciamo
un vettore \( \vec{u} = [u_1, \ldots, u_m]^{\tr} \in \mathbb{R}^m \) di moltiplicatori di Lagrange da utilizzare, in
combinazione con i vincoli che abbiamo rimosso, per costruire un contributo da aggiungere alla funzione obiettivo che ne
peggiora il valore per tutte le soluzioni che non soddisfano i vincoli che abbiamo eliminato. In questo modo, otteniamo
la nuova funzione obiettivo
\begin{equation}
    \min \; \sum_{j = 1}^n x_j + \sum_{i = 1}^m \Big[u_i \Big(1 - \sum_{j = 1}^n a_{ij}\, x_j\Big)\Big] =
    \min \; \sum_{j = 1}^n \Big[x_j\Big(1 - \sum_{i = 1}^m a_{ij}\, u_i \Big)\Big] + \sum_{i = 1}^m u_i
\end{equation}
e possiamo definire il rilassamento lagrangiano
\begin{equation}\label{eq:Lu}
    \mathcal{L}(\vec{u})\colon
    \left\{
    \begin{array}{ll}
        \min & \displaystyle\sum_{j=1}^n \Big[x_j \Big(1 - \sum_{i = 1}^m a_{ij}\, u_i\Big)\Big] + \sum_{i = 1}^m u_i
        \\[20pt]
             & x_j \in [0, 1] \qquad \forall j\colon 1 \leq j \leq n
    \end{array}\right.
\end{equation}
che può essere risolto scegliendo \( x_j = 1\) se e solo se
\begin{equation}
    \sum_{i=1}^m a_{ij}\, u_i > 1
\end{equation}
per ogni \( 1 \leq j \leq n \) e che fornisce, al variare di \( \vec{u} \in \mathbb{R}^n \), un limite inferiore al valore della funzione
obiettivo. A questo punto, possiamo definire il problema duale lagrangiano
\begin{equation}
    \max_{\vec{u} \,\geq\, 0}\; \mathcal{L}(\vec{u})
\end{equation}
con l'obiettivo di trovare la limitazione inferiore migliore, cioè quella più alta possibile. Inoltre, si può dimostrare
che il problema duale lagrangiano è un problema di ottimizzazione convessa, poiché la regione ammissibile è un insieme
convesso e la funzione obiettivo è concava, anche se non necessariamente differenziabile in tutti i punti in cui è
definita.

Utilizzando la notazione con matrici e vettori, possiamo riscrivere la formulazione \eqref{eq:Lu} nella forma compatta
\begin{equation}
    \mathcal{L}(\vec{u}) = \min_{\vec{x} \,\geq\,0}\, \{\vec{c}^{\tr}\vec{x} + \vec{u}^{\tr}(\vec{b}-\vec{A}\vec{x})\}
\end{equation}
ricordando che \( \vec{A} = [a_{ij}] \in \mathbb{R}^{m\times n}\), con \( \vec{x} = [x_1, \ldots, x_n]^{\tr} \in
\mathbb{R}^n \), \( \vec{c} = [1, 1, \ldots, 1]^{\tr} \in \mathbb{R}^n \) e \( \vec{b} = [1, 1, \ldots, 1]^{\tr} \in
\mathbb{R}^m \).
Infine, presentiamo una proprietà che sarà utile per l'implementazione dell'algoritmo risolutivo. Supponiamo che \(
\vec{x^{\star}} \) sia una soluzione ottima di \( \mathcal{L}(\vec{u^{\star}}) \), per qualche \( \vec{u^{\star}} \in
\mathbb{R}^m \). Allora il subgradiente di \( \mathcal{L}(\vec{u}) \) in \( \vec{u^{\star}} \) vale \(
\vec{s}_{\vec{u^{\star}}} = (\vec{b} - \vec{A}\vec{x^{\star}}) \in \mathbb{R}^m \).


\section{Matrice di riferimento di un'istanza}

Nel corso di questo capitolo, siamo partiti dalla specifica del problema del Set-Covering per ottenere la sua
formulazione matematica come problema di programmazione lineare intera. Successivamente abbiamo definito il rilassamento
lineare e ci siamo soffermati ad analizzare alcuni aspetti che lo riguardano, concludendo con la definizione del
problema duale e del rilassamento lagrangiano. L'ottenimento della formulazione \eqref{eq:scplr}, a partire da quella
presentata in \eqref{eq:defscp}, ci ha permesso di ottenere una formulazione in forma canonica per il rilassamento
lineare del Set-Covering. A questo punto, siamo finalmente pronti a capire la convenienza pratica di tale formulazione.
\`E infatti immediato osservare che descrivere il problema come in \eqref{eq:scplr} ci permette di caratterizzare una
specifica istanza semplicemente utilizzando la matrice binaria \( \vec{A} = [a_{ij}] \in \mathbb{R}^{m\times n}\),
evitando di dover specificare gli insiemi \( \mathcal{I} \) e \( \mathcal{F} \). Di conseguenza, per la creazione delle
istanze da utilizzare nelle prove sperimentali, ci limiteremo a generare matrici binarie di varie dimensioni e con
diversi valori di sparsità.
La possibilità di identificare un'instanza utilizzando la sua matrice di riferimento è molto utile, poiché permette di
limitare al minimo il numero dei parametri di ingresso richiesti dall'algoritmo risolutivo.

\subsection{Rappresentazione CSR}

Solitamente le istanze per il problema del Set-Covering hanno matrici di riferimento molto sparse, cioè matrici con
molti più zeri che uni. Per questo motivo, può essere conveniente cercare un modo intelligente di rappresentare la
matrice di riferimento, che non richieda di salvare in memoria i valori di tutti gli elementi. In particolare, visto
che la matrice è binaria, è sufficiente salvare solamente le informazioni relative agli elementi non nulli. In questo
modo possiamo risparmiare molta memoria e provare a sfruttare una rappresentazione compatta per ridurre il numero delle
operazioni da eseguire, evitando quelle che coinvolgono elementi nulli.


La rappresentazione CSR (\textit{Compressed Sparse Row}) permette di rappresentare una matrice sparsa \( \vec{A} \in
\mathbb{R}^{m\times n}\), con {\jbm nnz} elementi non nulli, utilizzando tre vettori che chiameremo {\jbm values,
indexes} e {\jbm pointers}. I vettori {\jbm values} e {\jbm indexes}, di lunghezza {\jbm nnz}, memorizzano i valori
degli elementi non nulli e i loro indici di colonna, rispettivamente. Il vettore {\jbm pointers}, di lunghezza \( m + 1
\), memorizza in ogni posizione l'indice che segnala l'inizio della riga corrispondente nei vettori {\jbm values} e
{\jbm pointers}. In altre parole, il valore dell'elemento {\jbm pointers[i]} è l'indice dei vettori {\jbm values} e
{\jbm indexes} che rappresenta l'inizio della riga {\jbm i} della matrice. Di conseguenza, per riferirci agli elementi
della riga {\jbm i} è sufficiente considerare i vettori {\jbm values} e {\jbm indexes} nell'intervallo di indici
delimitato da {\jbm pointers[i]} e {\jbm pointers[i + 1] - 1}.
